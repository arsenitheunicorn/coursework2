{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openingfile(filename):\n",
    "    wordlist = []\n",
    "    doc = docx.Document(filename)\n",
    "    for i in range(1, len(doc.paragraphs)):\n",
    "        for j in range(len(doc.paragraphs[i].runs)):\n",
    "            if doc.paragraphs[i].runs[j].bold == True:\n",
    "                if len(doc.paragraphs[i].runs[j].text) > 1:\n",
    "                    wordlist.append(doc.paragraphs[i].runs[j].text)\n",
    "    return wordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordlistfile(wl):\n",
    "    plain = '\\n'.join(wl)\n",
    "    with open('Wordlist.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write(plain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def afan():\n",
    "    clitics = []\n",
    "    with open('Wordlist.txt', 'r', encoding='utf-8') as f:\n",
    "        file = f.readlines()\n",
    "    for item in file:\n",
    "        item.lower()\n",
    "        if item.startswith('af') or item.startswith('an'):\n",
    "            clitics.append(item)\n",
    "    return clitics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def afanfile(clitics):\n",
    "    file = ''.join(clitics)\n",
    "    with open('afan.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## здесь я забираю из словаря все слова, которые начинаются на af- и an-\n",
    "wl = openingfile('WB_s.docx')\n",
    "wordlistfile(wl)\n",
    "cli = afan()\n",
    "afanfile(cli)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "greek = re.compile(u'[\\u1F00-\\u1FFE\\u0391-\\u03CF\\u03F4\\u0390]', re.UNICODE)\n",
    "def make_wb_better(greek):\n",
    "    doc = docx.Document('WB_s.docx')\n",
    "    fulltext = []\n",
    "    for para in doc.paragraphs:\n",
    "        parat = para.text\n",
    "        paratext = []\n",
    "        if re.search(greek, parat):\n",
    "            for run in para.runs:\n",
    "                if run.bold == True:\n",
    "                    goth_w = run.text\n",
    "                    paratext.append(goth_w)\n",
    "                elif re.search(greek, run.text):\n",
    "                    greek_w = run.text\n",
    "                    paratext.append(greek_w)\n",
    "        fulltext.append(paratext)\n",
    "    new_list = []\n",
    "    for paratext in fulltext:\n",
    "        tabd = '\\t'.join(paratext)\n",
    "        new_list.append(tabd)\n",
    "    return '\\n'.join(new_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_wb_txt(text):\n",
    "    with open('wb_try.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write(text)\n",
    "\n",
    "text = make_wb_better(greek)\n",
    "make_wb_txt(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from math import fabs\n",
    "greek = re.compile(u'[\\u1F00-\\u1FFE\\u0391-\\u03CF\\u03F4\\u0390]', re.UNICODE)\n",
    "def clean_wordlist(greek):\n",
    "    with open('wb_try.txt', 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    text = re.sub('[0-9.,*:]', '', text)\n",
    "    text = re.sub('\\s+([\\n\\t]\\S)', r'\\1', text)\n",
    "    text = re.sub('\\n', '\\t', text)\n",
    "    text = re.sub('(\\s)[A-Z][ηα]?\\s+', r'\\1', text)\n",
    "    blocks = text.split('\\t')\n",
    "    new_text = ''\n",
    "    for blo in blocks:\n",
    "        block = blo.strip()\n",
    "        if block:\n",
    "            n_grk = len(greek.findall(block))\n",
    "            n_ltn = len(re.findall('[A-Za-zþ]', block))\n",
    "            if fabs(n_grk - n_ltn) > 2:\n",
    "                if n_ltn > n_grk:\n",
    "                    new_text += '\\n'\n",
    "                    new_text += block\n",
    "                else:\n",
    "                    new_text += '\\t'\n",
    "                    new_text += block\n",
    "            elif len(block) <= 3:\n",
    "                if re.match(r'[A-Z\\u0391-\\u03A9\\u03F4]', block):\n",
    "                    new_text += ''\n",
    "                elif re.search('[a-zþ]', new_text[len(new_text)-3:]):\n",
    "                    if n_grk == 0 and re.search('[a-zþ]', block):\n",
    "                        new_text += block\n",
    "                    elif n_ltn == 0 and n_grk > 0:\n",
    "                        new_text += '\\t'\n",
    "                        new_text += block\n",
    "                elif greek.search(new_text[len(new_text)-3:]):\n",
    "                    if n_grk == 0 and re.search('[a-zþ]', block):\n",
    "                        new_text += '\\n'\n",
    "                        new_text += block\n",
    "                    elif n_ltn == 0 and n_grk > 0:\n",
    "                        new_text += ' '\n",
    "                        new_text += block\n",
    "            else:\n",
    "                print('последнее в буфере:')\n",
    "                print(new_text[len(new_text)-5:])\n",
    "                print('current block:')\n",
    "                print(block)\n",
    "                a = str(input('new_text += '))\n",
    "                while a != '':\n",
    "                    if a == '1':\n",
    "                        new_text += block\n",
    "                    else:\n",
    "                        new_text += a\n",
    "                    a = str(input('new_text += '))\n",
    "    with open('wb_try2.txt', 'w', encoding='utf-8') as f2:\n",
    "        f2.write(new_text)\n",
    "    return new_text\n",
    "                    \n",
    "#better_wb = clean_wordlist(greek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def look_for_prefix():\n",
    "    with open('wb_try2.txt', 'r', encoding='utf-8') as f:\n",
    "        ftext = f.read()\n",
    "    better = ftext[1:]\n",
    "    better = re.sub('-\\s?-', '', better)\n",
    "    words = better.split('\\n')\n",
    "    with_prefix = {}\n",
    "    for article in words:\n",
    "        article = re.sub(r'\\[', '', article)\n",
    "        article = re.sub(r'\\]', '', article)        \n",
    "        art = article.split('\\t', maxsplit=1)\n",
    "        if re.match('[Aa][nf]-?', art[0]):\n",
    "            try:\n",
    "                with_prefix[art[0]] = art[1].split('\\t')\n",
    "            except IndexError:\n",
    "                pass\n",
    "    return with_prefix\n",
    "        \n",
    "with_prefix = look_for_prefix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_prefixes(wb):\n",
    "    pfxs = {}\n",
    "    for word in sorted(wb.keys()):\n",
    "        if '-' in word:\n",
    "            pref = word.split('-')[0]\n",
    "            transl = wb[word]\n",
    "            if pref in pfxs:\n",
    "                pfxs[pref].extend(transl)\n",
    "            else:\n",
    "                pfxs[pref] = transl\n",
    "    with open('greek_pref.txt', 'r', encoding='utf-8') as g:\n",
    "        g2 = g.read()\n",
    "        pf_liste = re.split('-\\n', g2)\n",
    "    pfxs_up = {}\n",
    "    for key in pfxs:\n",
    "        for pf in pf_liste:\n",
    "            pfxs_up[key] = {pf: 0}\n",
    "    for key in pfxs:\n",
    "        for token in pfxs[key]:\n",
    "            if not token.startswith('('):\n",
    "                repl = \"[’''^<>-]\"\n",
    "                token = re.sub(repl, '', token)\n",
    "                winlist = []\n",
    "                if len(token) > 3:\n",
    "                    if ' ' not in token[:4]:\n",
    "                        tword = token\n",
    "                    else:\n",
    "                        tword = max(token.split(), key=len)\n",
    "                    i = 4\n",
    "                    while i > 0:\n",
    "                        if tword[:i] in pf_liste:\n",
    "                            winlist.append(tword[:i])\n",
    "                        i -= 1\n",
    "                    if winlist:\n",
    "                        for pf in winlist:\n",
    "                            if pf in pfxs_up[key]:\n",
    "                                pfxs_up[key][pf] += 1\n",
    "                            else:\n",
    "                                pfxs_up[key][pf] = 1\n",
    "                    else:\n",
    "                        pf = '??' + tword[:3]\n",
    "                        if pf in pfxs_up[key]:\n",
    "                            pfxs_up[key][pf] += 1\n",
    "                        else:\n",
    "                            pfxs_up[key][pf] = 1\n",
    "    with open('prfxs_use.txt', 'w', encoding='utf-8') as uf:\n",
    "        for kee in pfxs_up:\n",
    "            uf.write(kee + '-\\n')\n",
    "            ind = 1\n",
    "            for pf in sorted(pfxs_up[kee], key=pfxs_up[kee].get, reverse=True)[:10]:\n",
    "                uf.write(str(ind) + '\\t')\n",
    "                uf.write(pf + '\\t' + str(pfxs_up[kee][pf]) + '\\n')\n",
    "                ind += 1\n",
    "    return pfxs_up       \n",
    "    \n",
    "pfxs_up = sort_prefixes(with_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def look_for_piu():\n",
    "    with open('wb_try2.txt', 'r', encoding='utf-8') as f:\n",
    "        ftext = f.read()\n",
    "    better = ftext[1:]\n",
    "    better = re.sub('-\\s?-', '', better)\n",
    "    words = better.split('\\n')\n",
    "    found = {}\n",
    "    for article in words:\n",
    "        article = re.sub(r'\\[', '', article)\n",
    "        article = re.sub(r'\\]', '', article)        \n",
    "        art = article.split('\\t', maxsplit=1)\n",
    "        if re.match('þiu?', art[0]):\n",
    "            try:\n",
    "                found[art[0]] = art[1].split('\\t')\n",
    "            except IndexError:\n",
    "                pass\n",
    "    with open('piupiu.txt', 'w', encoding='utf-8') as pfp:\n",
    "        for item in sorted(found.keys()):\n",
    "            pfp.write(item + '\\t-->\\t' + ';'.join(found[item]))\n",
    "            pfp.write('\\n')\n",
    "\n",
    "look_for_piu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
